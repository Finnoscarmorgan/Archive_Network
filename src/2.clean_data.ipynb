{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This note book cleans the data for the network visuslisation.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14248, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the path to the folder containing the CSV files\n",
    "folder_path = 'data/Preprocessed_Facet_Datasets'\n",
    "\n",
    "# Get the list of files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_archival_data = pd.DataFrame()\n",
    "\n",
    "# Loop through the files and merge them\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        merged_archival_data = pd.concat([merged_archival_data, df], ignore_index=True)\n",
    "\n",
    "#print shape of merged data\n",
    "print(merged_archival_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13347, 11)\n"
     ]
    }
   ],
   "source": [
    "#use this one! just fix it!!! Fix duplicate records\n",
    "drop_dupes_df = merged_archival_data[merged_archival_data['PERSON'] != merged_archival_data['cleaned_author_name']]\n",
    "print(drop_dupes_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9529, 11)\n"
     ]
    }
   ],
   "source": [
    "#remove rows where there is just one word in the PERSON column. For example, 'Porter' and retain any entries with a space eg. \"M. Pitt\" or \"Marie Cowan\"\n",
    "# Remove rows where there is just one word in the PERSON column.\n",
    "# Retain entries with a space in the PERSON column, such as \"M. Pitt\" or \"Marie Cowan\".\n",
    "try:\n",
    "    drop_singles_df = drop_dupes_df[drop_dupes_df['PERSON'].str.contains(' ', na=False)]\n",
    "except KeyError as e:\n",
    "    print(\"Error: 'PERSON' column not found in the DataFrame.\")\n",
    "except AttributeError as e:\n",
    "    print(\"Error: 'PERSON' column is not a string type.\")\n",
    "\n",
    "print(drop_singles_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6625, 11)\n"
     ]
    }
   ],
   "source": [
    "#if the 'Bib ID' column and the 'PERSON' column are duplicated, remove the duplicate row\n",
    "#we don't care about if the same name is mentioned in the same catalogue record.\n",
    "\n",
    "try:\n",
    "    drop_person_dupes_df = drop_singles_df.drop_duplicates(subset=['Bib ID', 'PERSON'], keep='first')\n",
    "except KeyError as e:\n",
    "    print(\"Error: 'Bib ID' or 'PERSON' column not found in the DataFrame.\")\n",
    "\n",
    "print(drop_person_dupes_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6625 entries in the final dataframe\n",
      "there are 1106 individual archives from the NLA in the final dataframe\n",
      "there are 4776 different individuals mentioned in the catalogue meta-data for these archives in the final dataframe\n"
     ]
    }
   ],
   "source": [
    "print(\"there are {} entries in the final dataframe\".format(drop_person_dupes_df.shape[0]))\n",
    "print(\"there are {} individual archives from the NLA in the final dataframe\".format(drop_person_dupes_df['Bib ID'].nunique()))\n",
    "print(\"there are {} different individuals mentioned in the catalogue meta-data for these archives in the final dataframe\".format(drop_person_dupes_df['PERSON'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/16yc7hy13dn8gqzv70qdbg1h0000gn/T/ipykernel_40222/3672692138.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  drop_person_dupes_df['archival_extent_metres'] = drop_person_dupes_df['archival_extent']\n"
     ]
    }
   ],
   "source": [
    "#iterate through items in 'archival_extent' column and convert anything that is in cm to metres\n",
    "\n",
    "\n",
    "#create new column called 'archival_extent_metres' that is a copy of the 'archival_extent' column\n",
    "drop_person_dupes_df['archival_extent_metres'] = drop_person_dupes_df['archival_extent']\n",
    "\n",
    "#itterate through the rows in the 'archival_extent_metres' column and convert anything that is in cm to metres\n",
    "for index, row in drop_person_dupes_df.iterrows():\n",
    "    if 'cm' in row['archival_extent_metres']:\n",
    "        #split on ' cm' and take the first item in the list\n",
    "        extent = (row['archival_extent_metres'].split(' cm')[0])\n",
    "        #if extent contains letters (eg. '1 box') then set the value to 0\n",
    "        if any(c.isalpha() for c in extent):\n",
    "            drop_person_dupes_df.loc[index, 'archival_extent_metres'] = 0\n",
    "        else:\n",
    "            #convert to float\n",
    "            extent = float(extent)\n",
    "            #if extent is less than 1 then retain the value\n",
    "            if extent < 1:\n",
    "                drop_person_dupes_df.loc[index, 'archival_extent_metres'] = extent\n",
    "            else:\n",
    "                #convert to metres\n",
    "                drop_person_dupes_df.loc[index, 'archival_extent_metres'] = extent/100\n",
    "    elif 'm' in row['archival_extent_metres']:\n",
    "        #split on ' m' and take the first item in the list\n",
    "        extent = (row['archival_extent_metres'].split(' m')[0])\n",
    "        #if extent contains letters (eg. '1 box') then set the value to 0\n",
    "        if any(c.isalpha() for c in extent):\n",
    "            drop_person_dupes_df.loc[index, 'archival_extent_metres'] = 0\n",
    "        else:\n",
    "            #convert to float and set the value\n",
    "            drop_person_dupes_df.loc[index, 'archival_extent_metres'] = float(extent)\n",
    "    else:\n",
    "        #if any extent is not a float then set the value to 0\n",
    "        if any(c.isalpha() for c in row['archival_extent_metres']):\n",
    "            drop_person_dupes_df.loc[index, 'archival_extent_metres'] = 0\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where the 'archival_extent_metres' column is 0\n",
    "final_df = drop_person_dupes_df[drop_person_dupes_df['archival_extent_metres'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6094 entries in the final dataframe\n",
      "there are 967 individual archives from the NLA in the final dataframe\n",
      "there are 4444 different individuals mentioned in the catalogue meta-data for these archives in the final dataframe\n"
     ]
    }
   ],
   "source": [
    "print(\"there are {} entries in the final dataframe\".format(final_df.shape[0]))\n",
    "print(\"there are {} individual archives from the NLA in the final dataframe\".format(final_df['Bib ID'].nunique()))\n",
    "print(\"there are {} different individuals mentioned in the catalogue meta-data for these archives in the final dataframe\".format(final_df['PERSON'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the final dataframe to a csv file and a pickle file \n",
    "\n",
    "final_df.to_csv('data/final/processed_df.csv', index=False)\n",
    "final_df.to_pickle('data/final/processed_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Archive_Network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
