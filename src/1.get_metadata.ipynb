{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook retrieved the metadata to construct the network graph. The metadata was retrieved using the Trove API. The notebook is available as a Jupyter notebook.\n",
    "\n",
    "Code in this notebook has been adapted from the following sources:\n",
    "- Sherratt, Tim. (2023). trove-api-intro (version v1.0.0). Zenodo. https://doi.org/10.5281/zenodo.7545885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import fuzzysearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from IPython.display import HTML, display\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession()\n",
    "retries = Retry(total=10, backoff_factor=0.2, status_forcelist=[500, 502, 503, 504])\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API key is: \n"
     ]
    }
   ],
   "source": [
    "# This creates a variable called 'api_key', paste your key between the quotes\n",
    "api_key = \"\" # ADD YOUR API KEY HERE\n",
    "\n",
    "# Use an api key value from environment variables if it is available (useful for testing)\n",
    "if os.getenv(\"TROVE_API_KEY\"):\n",
    "    api_key = os.getenv(\"TROVE_API_KEY\")\n",
    "\n",
    "# This displays a message with your key\n",
    "print(\"Your API key is: {}\".format(api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_url = \"http://api.trove.nla.gov.au/v2/result\"\n",
    "# This creates a dictionary called 'params' and sets values for the API's mandatory parameters\n",
    "params = {\n",
    "    \"q\": \" \",  # Search for this keyword -- feel free to change!\n",
    "    \"zone\": \"collection\",\n",
    "    \"encoding\": \"json\",\n",
    "    \"l-format\": \"Unpublished\",\n",
    "    \"l-australian\": \"y\",\n",
    "    \"l-occupation\": \"Academics\", \n",
    "    \"key\": api_key,\n",
    "    \"n\": \"10000\",\n",
    "    \"include\": \"holdings\",\n",
    "    \"bulkHarvest\": \"true\",\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 473 records in the collection zone\n"
     ]
    }
   ],
   "source": [
    "response = s.get(api_url, params=params)\n",
    "data = response.json()\n",
    "total = int(data[\"response\"][\"zone\"][0][\"records\"][\"total\"])\n",
    "print(\"There are\", total, \"records in the collection zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "start = \"*\"\n",
    "params[\"n\"] = \"100\"  # Set the initial number of records per request to 100\n",
    "\n",
    "while start:\n",
    "    params[\"s\"] = start\n",
    "    response = s.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for record in data[\"response\"][\"zone\"][0][\"records\"][\"work\"]:\n",
    "        try:\n",
    "            if record[\"holding\"][0][\"nuc\"] == \"ANL\":\n",
    "                call_number = record[\"holding\"][0][\"callNumber\"][0]\n",
    "                if isinstance(call_number, str):\n",
    "                    #skip the item if it is a string\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                item = {\n",
    "                    \"trove_id\": record[\"id\"],\n",
    "                    \"title\": record[\"title\"],\n",
    "                    \"trove_url\": record[\"troveUrl\"],\n",
    "                    #\"holding\": record[\"holding\"][0][\"callNumber\"][0],\n",
    "                    \"holding\": record[\"holding\"][0][\"callNumber\"][0][\"localIdentifier\"],\n",
    "                    \"location\": record[\"holding\"][0][\"nuc\"]\n",
    "                }\n",
    "                # Add the item to the list of items\n",
    "                items.append(item)\n",
    "        except KeyError:\n",
    "            # Handle missing keys in the record\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        start = data[\"response\"][\"zone\"][0][\"records\"][\"nextStart\"]\n",
    "    except KeyError:\n",
    "        start = None\n",
    "\n",
    "    if len(items) >= total:\n",
    "        break\n",
    "\n",
    "    time.sleep(0.2)  # Add a delay between API requests to avoid overloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 items retrieved\n"
     ]
    }
   ],
   "source": [
    "print(len(items), \"items retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trove_id</th>\n",
       "      <th>title</th>\n",
       "      <th>trove_url</th>\n",
       "      <th>holding</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10201186</td>\n",
       "      <td>Papers of Otto Van der Sprenkel</td>\n",
       "      <td>https://trove.nla.gov.au/work/10201186</td>\n",
       "      <td>2788171</td>\n",
       "      <td>ANL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10347079</td>\n",
       "      <td>Papers of Dorothy Green</td>\n",
       "      <td>https://trove.nla.gov.au/work/10347079</td>\n",
       "      <td>1272711</td>\n",
       "      <td>ANL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10642789</td>\n",
       "      <td>Papers of John W. Burton</td>\n",
       "      <td>https://trove.nla.gov.au/work/10642789</td>\n",
       "      <td>2877417</td>\n",
       "      <td>ANL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10738203</td>\n",
       "      <td>Papers of Cameron Hazlehurst</td>\n",
       "      <td>https://trove.nla.gov.au/work/10738203</td>\n",
       "      <td>2912363</td>\n",
       "      <td>ANL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10738291</td>\n",
       "      <td>Papers of James Brigden</td>\n",
       "      <td>https://trove.nla.gov.au/work/10738291</td>\n",
       "      <td>2877442</td>\n",
       "      <td>ANL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trove_id                            title  \\\n",
       "0  10201186  Papers of Otto Van der Sprenkel   \n",
       "1  10347079          Papers of Dorothy Green   \n",
       "2  10642789         Papers of John W. Burton   \n",
       "3  10738203     Papers of Cameron Hazlehurst   \n",
       "4  10738291          Papers of James Brigden   \n",
       "\n",
       "                                trove_url  holding location  \n",
       "0  https://trove.nla.gov.au/work/10201186  2788171      ANL  \n",
       "1  https://trove.nla.gov.au/work/10347079  1272711      ANL  \n",
       "2  https://trove.nla.gov.au/work/10642789  2877417      ANL  \n",
       "3  https://trove.nla.gov.au/work/10738203  2912363      ANL  \n",
       "4  https://trove.nla.gov.au/work/10738291  2877442      ANL  "
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(items)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed item 1/282. Elapsed time: 0.88 seconds.\n",
      "Processed item 2/282. Elapsed time: 1.20 seconds.\n",
      "Processed item 3/282. Elapsed time: 1.65 seconds.\n",
      "Processed item 4/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 5/282. Elapsed time: 0.54 seconds.\n",
      "Processed item 6/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 7/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 8/282. Elapsed time: 0.98 seconds.\n",
      "Processed item 9/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 10/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 11/282. Elapsed time: 0.58 seconds.\n",
      "Processed item 12/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 13/282. Elapsed time: 0.96 seconds.\n",
      "Processed item 14/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 15/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 16/282. Elapsed time: 0.60 seconds.\n",
      "Processed item 17/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 18/282. Elapsed time: 1.45 seconds.\n",
      "Processed item 19/282. Elapsed time: 1.24 seconds.\n",
      "Processed item 20/282. Elapsed time: 0.78 seconds.\n",
      "Processed item 21/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 22/282. Elapsed time: 0.52 seconds.\n",
      "Processed item 23/282. Elapsed time: 0.52 seconds.\n",
      "Processed item 24/282. Elapsed time: 0.52 seconds.\n",
      "Processed item 25/282. Elapsed time: 0.52 seconds.\n",
      "Processed item 26/282. Elapsed time: 0.59 seconds.\n",
      "Processed item 27/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 28/282. Elapsed time: 0.54 seconds.\n",
      "Processed item 29/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 30/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 31/282. Elapsed time: 0.60 seconds.\n",
      "Processed item 32/282. Elapsed time: 0.74 seconds.\n",
      "Processed item 33/282. Elapsed time: 0.97 seconds.\n",
      "Processed item 34/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 35/282. Elapsed time: 1.70 seconds.\n",
      "Processed item 36/282. Elapsed time: 2.49 seconds.\n",
      "Processed item 37/282. Elapsed time: 1.03 seconds.\n",
      "Processed item 38/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 39/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 40/282. Elapsed time: 0.65 seconds.\n",
      "Processed item 41/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 42/282. Elapsed time: 0.66 seconds.\n",
      "Processed item 43/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 44/282. Elapsed time: 1.31 seconds.\n",
      "Processed item 45/282. Elapsed time: 1.08 seconds.\n",
      "Processed item 46/282. Elapsed time: 0.88 seconds.\n",
      "Processed item 47/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 48/282. Elapsed time: 0.76 seconds.\n",
      "Processed item 49/282. Elapsed time: 1.06 seconds.\n",
      "Processed item 50/282. Elapsed time: 1.40 seconds.\n",
      "Processed item 51/282. Elapsed time: 0.79 seconds.\n",
      "Processed item 52/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 53/282. Elapsed time: 1.10 seconds.\n",
      "Processed item 54/282. Elapsed time: 0.92 seconds.\n",
      "Processed item 55/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 56/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 57/282. Elapsed time: 0.78 seconds.\n",
      "Processed item 58/282. Elapsed time: 0.79 seconds.\n",
      "Processed item 59/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 60/282. Elapsed time: 0.82 seconds.\n",
      "Processed item 61/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 62/282. Elapsed time: 0.62 seconds.\n",
      "Processed item 63/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 64/282. Elapsed time: 1.15 seconds.\n",
      "Processed item 65/282. Elapsed time: 1.28 seconds.\n",
      "Processed item 66/282. Elapsed time: 1.41 seconds.\n",
      "Processed item 67/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 68/282. Elapsed time: 0.65 seconds.\n",
      "Processed item 69/282. Elapsed time: 0.80 seconds.\n",
      "Processed item 70/282. Elapsed time: 0.57 seconds.\n",
      "Processed item 71/282. Elapsed time: 0.79 seconds.\n",
      "Processed item 72/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 73/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 74/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 75/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 76/282. Elapsed time: 0.56 seconds.\n",
      "Processed item 77/282. Elapsed time: 0.87 seconds.\n",
      "Processed item 78/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 79/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 80/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 81/282. Elapsed time: 0.92 seconds.\n",
      "Processed item 82/282. Elapsed time: 1.90 seconds.\n",
      "Processed item 83/282. Elapsed time: 2.00 seconds.\n",
      "Processed item 84/282. Elapsed time: 1.56 seconds.\n",
      "Processed item 85/282. Elapsed time: 2.04 seconds.\n",
      "Processed item 86/282. Elapsed time: 1.19 seconds.\n",
      "Processed item 87/282. Elapsed time: 5.15 seconds.\n",
      "Processed item 88/282. Elapsed time: 1.10 seconds.\n",
      "Processed item 89/282. Elapsed time: 1.06 seconds.\n",
      "Processed item 90/282. Elapsed time: 1.00 seconds.\n",
      "Processed item 91/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 92/282. Elapsed time: 0.98 seconds.\n",
      "Processed item 93/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 94/282. Elapsed time: 1.19 seconds.\n",
      "Processed item 95/282. Elapsed time: 1.10 seconds.\n",
      "Processed item 96/282. Elapsed time: 0.88 seconds.\n",
      "Processed item 97/282. Elapsed time: 0.94 seconds.\n",
      "Processed item 98/282. Elapsed time: 2.84 seconds.\n",
      "Processed item 99/282. Elapsed time: 1.63 seconds.\n",
      "Processed item 100/282. Elapsed time: 1.01 seconds.\n",
      "Processed item 101/282. Elapsed time: 1.94 seconds.\n",
      "Processed item 102/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 103/282. Elapsed time: 0.94 seconds.\n",
      "Processed item 104/282. Elapsed time: 1.29 seconds.\n",
      "Processed item 105/282. Elapsed time: 1.32 seconds.\n",
      "Processed item 106/282. Elapsed time: 1.01 seconds.\n",
      "Processed item 107/282. Elapsed time: 0.60 seconds.\n",
      "Processed item 108/282. Elapsed time: 1.01 seconds.\n",
      "Processed item 109/282. Elapsed time: 0.76 seconds.\n",
      "Processed item 110/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 111/282. Elapsed time: 0.98 seconds.\n",
      "Processed item 112/282. Elapsed time: 1.49 seconds.\n",
      "Processed item 113/282. Elapsed time: 1.23 seconds.\n",
      "Processed item 114/282. Elapsed time: 1.99 seconds.\n",
      "Processed item 115/282. Elapsed time: 1.16 seconds.\n",
      "Processed item 116/282. Elapsed time: 1.93 seconds.\n",
      "Processed item 117/282. Elapsed time: 1.41 seconds.\n",
      "Processed item 118/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 119/282. Elapsed time: 0.65 seconds.\n",
      "Processed item 120/282. Elapsed time: 0.90 seconds.\n",
      "Processed item 121/282. Elapsed time: 0.66 seconds.\n",
      "Processed item 122/282. Elapsed time: 0.96 seconds.\n",
      "Processed item 123/282. Elapsed time: 1.15 seconds.\n",
      "Processed item 124/282. Elapsed time: 1.00 seconds.\n",
      "Processed item 125/282. Elapsed time: 0.98 seconds.\n",
      "Processed item 126/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 127/282. Elapsed time: 1.48 seconds.\n",
      "Processed item 128/282. Elapsed time: 1.51 seconds.\n",
      "Processed item 129/282. Elapsed time: 1.14 seconds.\n",
      "Processed item 130/282. Elapsed time: 2.11 seconds.\n",
      "Processed item 131/282. Elapsed time: 1.88 seconds.\n",
      "Processed item 132/282. Elapsed time: 1.15 seconds.\n",
      "Processed item 133/282. Elapsed time: 0.94 seconds.\n",
      "Processed item 134/282. Elapsed time: 1.17 seconds.\n",
      "Processed item 135/282. Elapsed time: 1.21 seconds.\n",
      "Processed item 136/282. Elapsed time: 1.55 seconds.\n",
      "Processed item 137/282. Elapsed time: 0.89 seconds.\n",
      "Processed item 138/282. Elapsed time: 1.01 seconds.\n",
      "Processed item 139/282. Elapsed time: 1.00 seconds.\n",
      "Processed item 140/282. Elapsed time: 0.81 seconds.\n",
      "Processed item 141/282. Elapsed time: 1.60 seconds.\n",
      "Processed item 142/282. Elapsed time: 1.21 seconds.\n",
      "Processed item 143/282. Elapsed time: 1.19 seconds.\n",
      "Processed item 144/282. Elapsed time: 0.82 seconds.\n",
      "Processed item 145/282. Elapsed time: 0.69 seconds.\n",
      "Processed item 146/282. Elapsed time: 0.58 seconds.\n",
      "Processed item 147/282. Elapsed time: 0.76 seconds.\n",
      "Processed item 148/282. Elapsed time: 0.56 seconds.\n",
      "Processed item 149/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 150/282. Elapsed time: 0.62 seconds.\n",
      "Processed item 151/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 152/282. Elapsed time: 0.65 seconds.\n",
      "Processed item 153/282. Elapsed time: 0.75 seconds.\n",
      "Processed item 154/282. Elapsed time: 0.57 seconds.\n",
      "Processed item 155/282. Elapsed time: 0.63 seconds.\n",
      "Processed item 156/282. Elapsed time: 0.60 seconds.\n",
      "Processed item 157/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 158/282. Elapsed time: 0.89 seconds.\n",
      "Processed item 159/282. Elapsed time: 1.50 seconds.\n",
      "Processed item 160/282. Elapsed time: 1.11 seconds.\n",
      "Processed item 161/282. Elapsed time: 0.91 seconds.\n",
      "Processed item 162/282. Elapsed time: 0.66 seconds.\n",
      "Processed item 163/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 164/282. Elapsed time: 0.51 seconds.\n",
      "Processed item 165/282. Elapsed time: 0.74 seconds.\n",
      "Processed item 166/282. Elapsed time: 0.62 seconds.\n",
      "Processed item 167/282. Elapsed time: 0.58 seconds.\n",
      "Processed item 168/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 169/282. Elapsed time: 0.57 seconds.\n",
      "Processed item 170/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 171/282. Elapsed time: 0.57 seconds.\n",
      "Processed item 172/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 173/282. Elapsed time: 0.76 seconds.\n",
      "Processed item 174/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 175/282. Elapsed time: 0.56 seconds.\n",
      "Processed item 176/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 177/282. Elapsed time: 1.26 seconds.\n",
      "Processed item 178/282. Elapsed time: 1.03 seconds.\n",
      "Processed item 179/282. Elapsed time: 4.79 seconds.\n",
      "Processed item 180/282. Elapsed time: 0.78 seconds.\n",
      "Processed item 181/282. Elapsed time: 0.81 seconds.\n",
      "Processed item 182/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 183/282. Elapsed time: 0.54 seconds.\n",
      "Processed item 184/282. Elapsed time: 0.58 seconds.\n",
      "Processed item 185/282. Elapsed time: 0.85 seconds.\n",
      "Processed item 186/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 187/282. Elapsed time: 1.03 seconds.\n",
      "Processed item 188/282. Elapsed time: 1.17 seconds.\n",
      "Processed item 189/282. Elapsed time: 1.11 seconds.\n",
      "Processed item 190/282. Elapsed time: 1.17 seconds.\n",
      "Processed item 191/282. Elapsed time: 0.81 seconds.\n",
      "Processed item 192/282. Elapsed time: 0.69 seconds.\n",
      "Processed item 193/282. Elapsed time: 0.78 seconds.\n",
      "Processed item 194/282. Elapsed time: 0.88 seconds.\n",
      "Processed item 195/282. Elapsed time: 0.83 seconds.\n",
      "Processed item 196/282. Elapsed time: 0.91 seconds.\n",
      "Processed item 197/282. Elapsed time: 0.83 seconds.\n",
      "Processed item 198/282. Elapsed time: 0.84 seconds.\n",
      "Processed item 199/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 200/282. Elapsed time: 0.54 seconds.\n",
      "Processed item 201/282. Elapsed time: 0.79 seconds.\n",
      "Processed item 202/282. Elapsed time: 1.31 seconds.\n",
      "Processed item 203/282. Elapsed time: 0.97 seconds.\n",
      "Processed item 204/282. Elapsed time: 1.42 seconds.\n",
      "Processed item 205/282. Elapsed time: 1.34 seconds.\n",
      "Processed item 206/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 207/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 208/282. Elapsed time: 0.94 seconds.\n",
      "Processed item 209/282. Elapsed time: 0.86 seconds.\n",
      "Processed item 210/282. Elapsed time: 0.55 seconds.\n",
      "Processed item 211/282. Elapsed time: 0.57 seconds.\n",
      "Processed item 212/282. Elapsed time: 0.99 seconds.\n",
      "Processed item 213/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 214/282. Elapsed time: 0.55 seconds.\n",
      "Processed item 215/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 216/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 217/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 218/282. Elapsed time: 0.94 seconds.\n",
      "Processed item 219/282. Elapsed time: 1.20 seconds.\n",
      "Processed item 220/282. Elapsed time: 1.41 seconds.\n",
      "Processed item 221/282. Elapsed time: 0.93 seconds.\n",
      "Processed item 222/282. Elapsed time: 0.65 seconds.\n",
      "Processed item 223/282. Elapsed time: 0.96 seconds.\n",
      "Processed item 224/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 225/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 226/282. Elapsed time: 0.62 seconds.\n",
      "Processed item 227/282. Elapsed time: 0.89 seconds.\n",
      "Processed item 228/282. Elapsed time: 0.82 seconds.\n",
      "Processed item 229/282. Elapsed time: 0.81 seconds.\n",
      "Processed item 230/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 231/282. Elapsed time: 0.79 seconds.\n",
      "Processed item 232/282. Elapsed time: 0.66 seconds.\n",
      "Processed item 233/282. Elapsed time: 0.64 seconds.\n",
      "Processed item 234/282. Elapsed time: 0.85 seconds.\n",
      "Processed item 235/282. Elapsed time: 1.23 seconds.\n",
      "Processed item 236/282. Elapsed time: 1.58 seconds.\n",
      "Processed item 237/282. Elapsed time: 1.29 seconds.\n",
      "Processed item 238/282. Elapsed time: 1.52 seconds.\n",
      "Processed item 239/282. Elapsed time: 1.82 seconds.\n",
      "Processed item 240/282. Elapsed time: 0.82 seconds.\n",
      "Processed item 241/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 242/282. Elapsed time: 0.83 seconds.\n",
      "Processed item 243/282. Elapsed time: 0.84 seconds.\n",
      "Processed item 244/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 245/282. Elapsed time: 0.76 seconds.\n",
      "Processed item 246/282. Elapsed time: 0.92 seconds.\n",
      "Processed item 247/282. Elapsed time: 1.09 seconds.\n",
      "Processed item 248/282. Elapsed time: 1.29 seconds.\n",
      "Processed item 249/282. Elapsed time: 1.26 seconds.\n",
      "Processed item 250/282. Elapsed time: 0.87 seconds.\n",
      "Processed item 251/282. Elapsed time: 1.02 seconds.\n",
      "Processed item 252/282. Elapsed time: 0.80 seconds.\n",
      "Processed item 253/282. Elapsed time: 0.73 seconds.\n",
      "Processed item 254/282. Elapsed time: 0.70 seconds.\n",
      "Processed item 255/282. Elapsed time: 0.83 seconds.\n",
      "Processed item 256/282. Elapsed time: 1.53 seconds.\n",
      "Processed item 257/282. Elapsed time: 0.83 seconds.\n",
      "Processed item 258/282. Elapsed time: 0.69 seconds.\n",
      "Processed item 259/282. Elapsed time: 0.68 seconds.\n",
      "Processed item 260/282. Elapsed time: 0.77 seconds.\n",
      "Processed item 261/282. Elapsed time: 1.13 seconds.\n",
      "Processed item 262/282. Elapsed time: 1.07 seconds.\n",
      "Processed item 263/282. Elapsed time: 1.09 seconds.\n",
      "Processed item 264/282. Elapsed time: 0.80 seconds.\n",
      "Processed item 265/282. Elapsed time: 0.69 seconds.\n",
      "Processed item 266/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 267/282. Elapsed time: 0.71 seconds.\n",
      "Processed item 268/282. Elapsed time: 0.67 seconds.\n",
      "Processed item 269/282. Elapsed time: 0.99 seconds.\n",
      "Processed item 270/282. Elapsed time: 0.88 seconds.\n",
      "Processed item 271/282. Elapsed time: 0.61 seconds.\n",
      "Processed item 272/282. Elapsed time: 0.62 seconds.\n",
      "Processed item 273/282. Elapsed time: 0.82 seconds.\n",
      "Processed item 274/282. Elapsed time: 0.72 seconds.\n",
      "Processed item 275/282. Elapsed time: 0.69 seconds.\n",
      "Processed item 276/282. Elapsed time: 0.86 seconds.\n",
      "Processed item 277/282. Elapsed time: 1.06 seconds.\n",
      "Processed item 278/282. Elapsed time: 2.16 seconds.\n",
      "Processed item 279/282. Elapsed time: 0.97 seconds.\n",
      "Processed item 280/282. Elapsed time: 0.63 seconds.\n",
      "Processed item 281/282. Elapsed time: 0.58 seconds.\n",
      "Processed item 282/282. Elapsed time: 5.25 seconds.\n",
      "Total elapsed time: 273.37 seconds.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def collect_archive_metadata(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the 'div' with class 'record'\n",
    "    record_div = soup.find('div', class_='record')\n",
    "\n",
    "    # Find the title of the record and strip the text\n",
    "    title_element = soup.find('h5', attrs={'class': 'header_title'}).text.strip()\n",
    "\n",
    "    # Find the table with metadata information\n",
    "    table = soup.find('table', class_='recDisplay')\n",
    "\n",
    "    # Find all <tr> elements within the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    manuscript_metadata = {}  # Dictionary to store the extracted data\n",
    "\n",
    "    for row in rows:\n",
    "        th = row.find('th')  # Find the <th> element\n",
    "        td = row.find('td')  # Find the <td> element\n",
    "        if th is not None and td is not None:\n",
    "            key = th.text.strip()\n",
    "            value = td.text.strip()\n",
    "            manuscript_metadata[key] = value\n",
    "\n",
    "    # Create a new dictionary to store title_element as key and manuscript_metadata as value\n",
    "    data = {title_element: manuscript_metadata}\n",
    "    return data\n",
    "\n",
    "def construct_NLA_url(df):\n",
    "    NLA_url = {}  # Initialize the dictionary\n",
    "    for i in df['holding']:\n",
    "        # Make sure the holding number is a string\n",
    "        i = str(i)\n",
    "        NLA_url[i] = \"https://catalogue.nla.gov.au/Record/\" + i\n",
    "    return NLA_url\n",
    "\n",
    "\n",
    "NLA_archive_metadata = construct_NLA_url(df)\n",
    "\n",
    "#create a new dictionary to store the metadata\n",
    "total_results_metadata = {}\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "for index, (holding, url) in enumerate(NLA_archive_metadata.items()):\n",
    "    metadata_start_time = time.time()  # Start the timer for each item\n",
    "    metadata = collect_archive_metadata(url)\n",
    "    total_results_metadata.update(metadata)\n",
    "    metadata_end_time = time.time()  # Stop the timer for each item\n",
    "    elapsed_time = metadata_end_time - metadata_start_time\n",
    "    print(f\"Processed item {index+1}/{len(NLA_archive_metadata)}. Elapsed time: {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "end_time = time.time()  # Stop the timer\n",
    "total_elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Total elapsed time: {total_elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save total_results_metadata as a json file\n",
    "with open('total_results_metadata.json', 'w') as f:\n",
    "    json.dump(total_results_metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dictionary that will store named entities as values under title of the record as key.\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# COMMENT OUT IF YOU ARE LOADING IN JSON FILE\n",
    "#with open('ADD FILE NAME HERE') as file:\n",
    "#    data = json.load(file)\n",
    "\n",
    "# Dictionary to store named entitites with the same key as total_results_metadata dictionary\n",
    "same_key_entities = {}\n",
    "\n",
    "# load in dictionary of total_results_metadata\n",
    "for entry_key, entry_value in total_results_metadata.items():\n",
    "    same_key = entry_key\n",
    "\n",
    "#for entry_key, entry_value in data.items(): # COMMENT THIS OUT IF LOADING IN JSON FILE\n",
    "#for entry_key, entry_value in total_results_metadata.items():\n",
    "#    same_key = entry_key  \n",
    "\n",
    "    named_entities = {\"ORGANIZATION\": [], \"LOCATION\": [], \"PERSON\": []}  # Use lists to store named entities by type\n",
    "    for field in ['Description', 'Summary', 'Notes', 'Biography/History']:\n",
    "        text = entry_value.get(field, '')  # Get the text from the current field (default to empty string if field is missing)\n",
    "        doc = nlp(text)  # Process the text with spaCy's NLP pipeline\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ in named_entities:\n",
    "                named_entities[entity.label_].append(entity.text)  # Add the named entity to the corresponding list\n",
    "\n",
    "    same_key_entities[same_key] = named_entities  # Store the named entities dictionary using the same key\n",
    "\n",
    "# Save to JSON file\n",
    "with open('named_entities.json', 'w') as file:\n",
    "    json.dump(same_key_entities, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge same_key_entities into total_results_metadata dictionary and save as new dictionary called total_results_metadata_with_entities\n",
    "total_results_metadata_with_entities = {}\n",
    "\n",
    "for entry_key, entry_value in total_results_metadata.items():\n",
    "    same_key = entry_key\n",
    "    named_entities = same_key_entities.get(same_key, {})\n",
    "    entry_value.update(named_entities)\n",
    "    total_results_metadata_with_entities[same_key] = entry_value\n",
    "\n",
    "#save total_results_metadata_with_entities as a json file for safekeeping\n",
    "with open('total_results_metadata_with_entities.json', 'w') as f:\n",
    "    json.dump(total_results_metadata_with_entities, f, indent=4)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load the data from json_one\\nwith open('authors_total_results_same_key_entitiesTRYFIX.json') as file:\\n    json_one_data = json.load(file)\\n\\n# Load the data from json_two\\nwith open('authors_total_results_metadata.json') as file:\\n    json_two_data = json.load(file)\\n\\n# Iterate over each entry in json_one_data\\nfor key, entities in json_one_data.items():\\n    # Check if the key exists in json_two_data\\n    if key in json_two_data:\\n        # Merge the information from json_one_data onto json_two_data\\n        json_two_data[key].update(entities)\\n\\n# Save the updated json_two_data to a file\\nwith open('merged.json', 'w') as file:\\n    json.dump(json_two_data, file, indent=4)\\n\\n\""
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IF YOU ARE WORKING FROM JSON FILES YOU CAN USE THE SCRIPT TO MERGE THE TWO FILES\n",
    "\"\"\"\n",
    "# Load the data from json_one\n",
    "with open('authors_total_results_same_key_entitiesTRYFIX.json') as file:\n",
    "    json_one_data = json.load(file)\n",
    "\n",
    "# Load the data from json_two\n",
    "with open('authors_total_results_metadata.json') as file:\n",
    "    json_two_data = json.load(file)\n",
    "\n",
    "# Iterate over each entry in json_one_data\n",
    "for key, entities in json_one_data.items():\n",
    "    # Check if the key exists in json_two_data\n",
    "    if key in json_two_data:\n",
    "        # Merge the information from json_one_data onto json_two_data\n",
    "        json_two_data[key].update(entities)\n",
    "\n",
    "# Save the updated json_two_data to a file\n",
    "with open('merged.json', 'w') as file:\n",
    "    json.dump(json_two_data, file, indent=4)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TERMPORARY CODE BLOCK TO RESOLVE ISSUES\n",
    "\n",
    "import re\n",
    "\n",
    "for entry_key, entry_value in total_results_metadata_with_entities.items():\n",
    "    # Retrieve the value for 'Author' key, or provide an empty string if the key is not found\n",
    "    author_name = entry_value.get('Author', '')\n",
    "    # Remove numeric values from Author field and replace with an empty string\n",
    "    cleaned_author_name = re.sub(r'\\d+', '', author_name)\n",
    "    # Split on the first comma and reverse the order of the list\n",
    "    cleaned_author_name = cleaned_author_name.split(',', 1)[::-1]\n",
    "    # Join the list back together with a space between the two elements\n",
    "    cleaned_author_name = ' '.join(cleaned_author_name)\n",
    "    # Remove commas from the end of the Author name and any non-alphanumeric characters\n",
    "    cleaned_author_name = re.sub(r'[^a-zA-Z0-9 ]+', '', cleaned_author_name).rstrip(',')\n",
    "    # Remove any double spaces\n",
    "    cleaned_author_name = re.sub(r'\\s+', ' ', cleaned_author_name)\n",
    "    # Remove any leading or trailing spaces\n",
    "    cleaned_author_name = cleaned_author_name.strip()\n",
    "    # Add the cleaned_author_name to the dictionary as a new value under the key 'cleaned_author_name'\n",
    "    entry_value['cleaned_author_name'] = cleaned_author_name\n",
    "\n",
    "# Now you can proceed with further processing or create a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract information on archival extent for each archive from \"Description\"\n",
    "# split on last '\\n' and take the last element of the list\n",
    "\n",
    "for entry_key, entry_value in total_results_metadata_with_entities.items():\n",
    "    archival_extent = entry_value['Description'].rsplit('\\n', 1)[-1]\n",
    "    #split on brackets and take the first element of the list\n",
    "    archival_extent = archival_extent.split('(', 1)[0]\n",
    "    #remove trailing whitespace\n",
    "    archival_extent = archival_extent.rstrip()\n",
    "    entry_value['archival_extent'] = archival_extent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dictionary to dataframe but only include Bib ID, cleaned_author_name, archival_extent, and named entities.\n",
    "#this will be used to create the network visualisation\n",
    "#explode the named entities so that each named entity is in a separate row with the same Bib ID, cleaned_author_name, and archival_extent\n",
    "#this will allow us to create a network visualisation with the named entities as nodes and the Bib ID, cleaned_author_name, and archival_extent as attributes\n",
    "\n",
    "#convert dictionary to dataframe\n",
    "entity_new_df = pd.DataFrame.from_dict(total_results_metadata_with_entities, orient='index', columns=['Bib ID', 'Author', 'Description', 'Summary', 'Notes', 'Biography/History', 'ORGANIZATION', 'LOCATION', 'PERSON', 'cleaned_author_name', 'archival_extent'])\n",
    "#explode the named entities so that each named entity is in a separate row with the same Bib ID, cleaned_author_name, and archival_extent\n",
    "#this will allow us to create a network visualisation with the named entities as nodes and the Bib ID, cleaned_author_name, and archival_extent as attributes\n",
    "explode_entity_new_df = entity_new_df.explode('PERSON')\n",
    "#save explode_entity_new_df as a csv file for safekeeping\n",
    "explode_entity_new_df.to_csv('explode_entity_new_df.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
